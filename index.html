<!DOCTYPE html>
<!-- TODO: Add google analytics. -->
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-99ZPPJ0J6N"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-99ZPPJ0J6N');
    </script>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
      rel="stylesheet"
    />
    <link
      href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="./css/main.css" />
    <title>SPML Seminar</title>
  </head>
  <body>
    <div class="container">
      <div class="logo">
        <img src="./assets/logo.png" />
        <hr />
      </div>

      <h2>About the seminar series</h2>
      <!-- <p>
        If you looking for an online seminar for the latest updates on security
        & privacy research in Machine learning, then you have come to the right
        place.
      </p> -->

      <p>
        The motivation for the seminar is to build a platform to discuss and
        disseminate the progress made by the community in solving some of the
        core challenges. We intend to host weekly talks from leading researchers
        in both academia and industry. Each session will be split into a talk
        (40 mins) followed by a Q&A + short discussion session (20 mins).
      </p>

      <p>
        <b
          ><span class="colored">Timing:</span> Every Tuesday at 1pm Eastern
          Time (Virtual talks)</b
        >
      </p>

      <p>
        We recommend following two steps to get details about talks (including
        zoom links):
        <ul>
          <li>
            <b>Step-1:</b> <em>Join the mailing list</em> (<a
              href="https://groups.google.com/forum/#!forum/spml-seminars/join"
              target="_blank"
              ><b>here</b></a
            >) to receive regular updates.
          </li>
          <li>
            <b>Step-2:</b> <em>Sync SPML calendar</em> (<a
              href="https://calendar.google.com/calendar/u/0?cid=N2FwbTVxYzJsOGM2bXBiNGY4am1oMjNsdGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ"
              target="_blank"
              ><b>here</b></a
            >) to never miss a talk.
          </li>
        </ul>
      </p>

      <!-- <p>
        Only following speaker's consideration, we will be able to make the
        recording public after tha talk, thus we highly recommend attending
        them.
      </p> -->

      <h2>Upcoming talks</h2>
      <!-- TODO: Set up way to add talk to calendar. Find a way to highlight next talk. -->
      <div class="talks">
        <div class="talk grid">
          <div class="pane leftPane">
            <img src="./assets/speakers/tom_headshot.jpeg" />
          </div>
          <div class="pane rightPane">
            <div class="date">
              07 June 2022
              <!-- <div id="clock_tom" class="clock">
                (
                <div><span class="days"></span> days</div>
                <div><span class="hours"></span> hrs</div>
                )
              </div> -->
            </div>
            <div class="name">
              <a href="http://www.cs.umd.edu/~tomg/" target="_blank"
                ><b>Tom Goldstein</b></a
              >
              <span class="affiliation">(University of Maryland)</span>
            </div>
            <div class="title">Just how private is federated learning?</div>
            <details>
              <summary><span class="droptext">Abstract & Bio</span></summary>
              <p><strong>Abstract:</strong> Federated learning is often touted as a training paradigm that preserves user privacy.  In this talk, I’ll discuss ways that federated protocols leak user information, and ways that malicious actors can exploit federated protocols to scrape information from
                users.  If time permits, I’ll also discuss how recent advances in data poisoning can manipulate datasets to preserve privacy by preventing data from being used for model training.</p>
              <p><strong>Bio:</strong> Tom Goldstein is the Perotto Associate Professor of Computer Science at the University of Maryland.  His research lies at the intersection of machine learning and optimization, and targets applications in computer vision and signal processing. Before joining
                the faculty at Maryland, Tom completed his PhD in Mathematics at UCLA, and was a research scientist at Rice University and Stanford University. Professor Goldstein has been the recipient of several awards, including SIAM’s DiPrima Prize, a DARPA Young Faculty
                Award, a JP Morgan Faculty award, and a Sloan Fellowship.</p>
            </details>
            <div class="flexrow">
              <a href="https://princeton.zoom.us/j/95819003743" target="_blank"
                ><div class="friterm">
                  Zoom link: <i class="fa fa-video-camera zoom icon"></i>
                </div>
              </a>
              <a href="https://youtu.be/jiLg5HG3hMk" target="_blank"
                ><div class="friterm">
                  Recording:
                  <i class="fa fa-brands fa-youtube-play youtube icon"></i>
                </div>
              </a>
            </div>
          </div>
        </div>

        <div class="talk grid">
          <div class="pane leftPane">
            <img src="./assets/speakers/bo_li_headshot.png" />
          </div>
          <div class="pane rightPane">
            <div class="date">
              14 June 2022
              <!-- <div id="clock_bo" class="clock">
                (
                <div><span class="days"></span> days</div>
                <div><span class="hours"></span> hrs</div>
                )
              </div> -->
            </div>
            <div class="name">
              <a
                href="https://aisecure.github.io/"
                target="_blank"
                ><b>Bo Li</b></a
              >
              <span class="affiliation"
                >(University of Illinois Urbana-Champaign)</span
              >
            </div>
            <div class="title">Trustworthy Machine Learning: Robustness, Privacy, Generalization, and their Interconnections</div>
            <details>
              <summary><span class="droptext">Abstract & Bio</span></summary>
              <p><strong>Abstract:</strong> Advances in machine learning have led to the rapid and widespread deployment of
                learning based methods in safety-critical applications, such as autonomous driving and
                medical healthcare. Standard machine learning systems, however, assume that training
                and test data follow the same, or similar, distributions, without explicitly considering
                active adversaries manipulating either distribution. For instance, recent work has
                demonstrated that motivated adversaries can circumvent anomaly detection or other
                machine learning models at test-time through evasion attacks, or can inject well-crafted
                malicious instances into training data to induce errors during inference through poisoning
                attacks. Such distribution shift could also lead to other trustworthiness issues such as
                generalization. In this talk, I will describe different perspectives of trustworthy machine
                learning, such as robustness, privacy, generalization, and their underlying
                interconnections. I will focus on a certifiably robust learning approach based on statistical
                learning with logical reasoning as an example, and then discuss the principles towards
                designing and developing practical trustworthy machine learning systems with
                guarantees, by considering these trustworthiness perspectives in a holistic view.</p>
              <p><strong>Bio:</strong> Dr. Bo Li is an assistant professor in the Department of Computer Science at the
                University of Illinois at Urbana–Champaign. She is the recipient of the MIT Technology
                Review TR-35 Award, Alfred P. Sloan Research Fellowship, NSF CAREER Award,
                IJCAI Computer and Thought Award, Dean's Award for Excellence in Research, C.W.
                Gear Outstanding Junior Faculty Award, Intel Rising Star award, Symantec Research
                Labs Fellowship, Rising Star Award, Research Awards from Tech companies such as
                Amazon, Facebook, Intel, and IBM, and best paper awards at several top machine
                learning and security conferences. Her research focuses on both theoretical and practical
                aspects of trustworthy machine learning, security, machine learning, privacy, and game
                theory. She has designed several scalable frameworks for trustworthy machine learning
                and privacy-preserving data publishing systems. Her work has been featured by major
                publications and media outlets such as Nature, Wired, Fortune, and New York Times.</p>
            </details>
            <div class="flexrow">
              <a href="https://princeton.zoom.us/j/97905349789" target="_blank"
                ><div class="friterm">
                  Zoom link: <i class="fa fa-video-camera zoom icon"></i>
                </div>
              </a>
              <a href="./assets/dummy.html" target="_blank"
                ><div class="friterm">
                  <!-- Recording:
                  <i class="fa fa-brands fa-youtube-play youtube icon"></i> -->
                </div>
              </a>
            </div>
          </div>
        </div>

        <div class="talk grid">
          <div class="pane leftPane">
            <img src="./assets/speakers/ben_zhao_headshot.png" />
          </div>
          <div class="pane rightPane">
            <div class="date">
              21 June 2022
              <!-- <div id="clock_ben" class="clock">
                (
                <div><span class="days"></span> days</div>
                <div><span class="hours"></span> hrs</div>
                )
              </div> -->
            </div>
            <div class="name">
              <a
                href="https://people.cs.uchicago.edu/~ravenben/"
                target="_blank"
                ><b>Ben Y. Zhao</b></a
              >
              <span class="affiliation">(University of Chicago)</span>
            </div>
            <div class="title">Adversarial Robustness and Forensics in Deep Neural Networks</div>
            <details>
              <summary><span class="droptext">Abstract & Bio</span></summary>
              <p><strong>Abstract:</strong> Despite their tangible impact on a wide range of real world applications, deep neural networks are known to be vulnerable to numerous attacks, including inference time attacks based on adversarial perturbations, as well as training time attacks such as backdoors. The security community has done extensive work to explore both attacks and defenses, only to produce a seemingly endless cat-and-mouse game.</p>
              <p>
              In this talk, I will talk about some of our recent work into adversarial robustness for DNNs, with a focus on ML digital forensics. I start by summarizing some of our recent projects at UChicago SAND Lab covering both sides of the attack/defense struggle, including honeypot defenses (CCS 2020) and physical domain poison attacks (CVPR 2021). Our  experiences in these projects motivated us to seek a broader, more realistic view towards adversarial robustness, beyond the current static, binary views of attack and defense. Like real world security systems, we take a pragmatic view that given sufficient incentive and resources, attackers will eventually succeed in compromising DNN systems. Just as in traditional security realms, digital forensics tools can serve dual purposes: identifying the sources of the compromise so that they can be mitigated, while also providing a strong deterrent against future attackers.  I will present results from our first paper in this space (Usenix Security 2022), specifically addressing forensics for poisoning attacks against DNNs, and show how we can trace back corrupted models to specific subsets of training data responsible for the corruption. Our approach builds up on ideas from model unlearning, and succeeds with high precision/recall for both dirty- and clean-label attacks.
              </p>
              <p><strong>Bio:</strong> Ben Zhao is Neubauer Professor of Computer Science at University of Chicago. Prior to joining UChicago, he held the position of Professor of Computer Science at UC Santa Barbara. He completed his Ph.D. at U.C. Berkeley (2004), and B.S. from Yale (1997). He is an ACM Fellow, and a recipient of the NSF CAREER award, MIT Technology Review's TR-35 Award (Young Innovators Under 35), ComputerWorld Magazine's Top 40 Technology Innovators award, IEEE ITC Early Career Award, and Google Faculty awards. His work has been covered by media outlets such as New York Times, Boston Globe, LA Times, MIT Tech Review, Wall Street Journal, Forbes, Fortune, CNBC, MSNBC, New Scientist, and Slashdot.  He has published extensively in areas of security and privacy, machine learning, networking, and HCI. He served as TPC (co)chair for the World Wide Web conference (WWW 2016) and ACM Internet Measurement Conference (IMC 2018). He also serves on the steering committee for HotNets, and was general co-chair for HotNets 2020. </p>
            </details>
            <div class="flexrow">
              <a href="https://princeton.zoom.us/j/93579877646" target="_blank"
                ><div class="friterm">
                  Zoom link: <i class="fa fa-video-camera zoom icon"></i>
                </div>
              </a>
              <a href="./assets/dummy.html" target="_blank"
                ><div class="friterm">
                  <!-- Recording:
                  <i class="fa fa-brands fa-youtube-play youtube icon"></i> -->
                </div>
              </a>
            </div>
          </div>
        </div>

        <div class="talk grid">
          <div class="pane leftPane">
            <img src="./assets/speakers/kamalika_headshot.jpeg" />
          </div>
          <div class="pane rightPane">
            <div class="date">
              28 June 2022
              <!-- <div id="clock_kam" class="clock">
                (
                <div><span class="days"></span> days</div>
                <div><span class="hours"></span> hrs</div>
                )
              </div> -->
            </div>
            <div class="name">
              <a href="https://cseweb.ucsd.edu/~kamalika/" target="_blank"
                ><b>Kamalika Chaudhuri</b></a
              >
              <span class="affiliation">(UCSD and Meta AI)</span>
            </div>
            <div class="title">Talk title: Tbd</div>
            <div class="droptext">Abstract & Bio</div>
            <!-- <div class="flexrow">
              <a href="./assets/dummyzoom.html" target="_blank"
                ><div class="friterm">
                  Zoom link: <i class="fa fa-video-camera zoom icon"></i>
                </div>
              </a>
              <a href="./assets/dummy.html" target="_blank"
                ><div class="friterm">
                  Recording:
                  <i class="fa fa-brands fa-youtube-play youtube icon"></i>
                </div>
              </a>
            </div> -->
          </div>
        </div>

        <div class="talk grid">
          <div class="pane leftPane">
            <img src="./assets/speakers/alex_headshot.jpeg" />
          </div>
          <div class="pane rightPane">
            <div class="date">
              05 July 2022
              <!-- <div id="clock_alex" class="clock">
                (
                <div><span class="days"></span> days</div>
                <div><span class="hours"></span> hrs</div>
                )
              </div> -->
            </div>
            <div class="name">
              <a href="https://alexandresablayrolles.github.io/" target="_blank"
                ><b>Alexandre Sablayrolles</b></a
              >
              <span class="affiliation">(Meta AI)</span>
            </div>
            <div class="title">Talk title: Tbd</div>
            <div class="droptext">Abstract & Bio</div>
            <!-- <div class="flexrow">
              <a href="./assets/dummyzoom.html" target="_blank"
                ><div class="friterm">
                  Zoom link: <i class="fa fa-video-camera zoom icon"></i>
                </div>
              </a>
              <a href="./assets/dummy.html" target="_blank"
                ><div class="friterm">
                  Recording:
                  <i class="fa fa-brands fa-youtube-play youtube icon"></i>
                </div>
              </a>
            </div> -->
          </div>
        </div>
      </div>

      <p>
        <b>Organizers:</b>
        <a href="https://vsehwag.github.io/">Vikash Sehwag</a> (Princeton),
        <a href="https://cihangxie.github.io/">Cihang Xie </a>(UCSC), and
        <a href="https://jamiehay.es/">Jamie Hayes </a>(Deepmind)
        <br />
        <b>Advisory committe:</b>
        <a href="https://www.princeton.edu/~pmittal/">Prateek Mittal</a>
        (Princeton), <a href="https://www.comp.nus.edu.sg/~reza/">Reza Shokri</a> (NUS)
      </p>

      <p>
        You can reach us at
        <span class="email">vvikash [at] princeton.edu</span> or
        <span class="email">cihangxie306 [at] gmail.com</span> or
        <span class="email">jamhay [at] google.com</span> for questions or
        suggestions related to the seminar.
      </p>
    </div>
  </body>
  <script src="./css/main.js"></script>
</html>
